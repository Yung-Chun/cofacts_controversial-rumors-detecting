{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e4012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from igraph import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import io\n",
    "import requests\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from zhon.hanzi import characters\n",
    "import math\n",
    "from opencc import OpenCC \n",
    "cc = OpenCC('s2t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e87c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = #deleted for security reasons\n",
    "out_path = #deleted for security reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fff7aa",
   "metadata": {},
   "source": [
    "# catagorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cef19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_all_chinese(strs):\n",
    "    for _char in strs:\n",
    "        if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf846101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catagorize(segment, key_word_list):\n",
    "    result = 'no'\n",
    "    if any(kw in segment and kw for kw in key_word_list)==True:\n",
    "        result = 'yes'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba15a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words_1 = ['疫情', '隔離', '疫苗', '防疫' ,'病毒', '新冠' , '肺炎']\n",
    "key_words_2 = ['共存', '清零']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a286956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "end = time.process_time()\n",
    "\n",
    "#匯入已經斷詞好的檔案\n",
    "print('loading...')\n",
    "with open(root_path/'cofacts_wordseg_20220322-20220422.json' , 'r', encoding='big5') as reader:\n",
    "    data = json.loads(reader.read())\n",
    "print(len(data))\n",
    "\n",
    "print('cleaning...')\n",
    "for idx, doc in enumerate(data):\n",
    "    #clean_segment\n",
    "    segment = data[idx]['segment']\n",
    "    clean_segment = []\n",
    "    for s in segment:\n",
    "        clean_s = cc.convert(s.replace('\\n',''))\n",
    "        if is_all_chinese(clean_s)==True and len(clean_s)>1:\n",
    "            clean_segment.append(clean_s)\n",
    "\n",
    "    data[idx]['clean_segment'] = clean_segment\n",
    "\n",
    "    #catagorize\n",
    "    covid_text = catagorize(clean_segment, key_words_1) \n",
    "    if covid_text == 'yes':\n",
    "        coexist_text = catagorize(clean_segment, key_words_2)\n",
    "    else:\n",
    "        coexist_text = None\n",
    "    data[idx]['covid'] = covid_text\n",
    "    data[idx]['coexist'] = coexist_text\n",
    "\n",
    "reader.close()\n",
    "\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_csv(root_path/'cofacts_clean_wordseg_20220322-20220422.csv')\n",
    "\n",
    "print(\"This time is being calculated\")\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dedae",
   "metadata": {},
   "source": [
    "# HAC+KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a58c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算詞頻，產生詞頻矩陣\n",
    "def vectorize(tokenList):\n",
    "    stringifyToks = [\" \".join(t) for t in tokenList]\n",
    "    cv = CountVectorizer(binary=True, min_df=1, max_df=0.9)\n",
    "    wordVector_csr = cv.fit_transform(stringifyToks) #計算詞頻\n",
    "    return wordVector_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40edae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(mat):\n",
    "    #壓縮稀疏矩陣\n",
    "    if not scipy.sparse.issparse(mat):\n",
    "        mat = scipy.sparse.csr_matrix(mat)\n",
    "    mat = mat.astype(float)\n",
    "\n",
    "    intrsct = mat * mat.T\n",
    "\n",
    "    # for rows\n",
    "    row_sums = mat.getnnz(axis=1)\n",
    "    nnz_i = np.repeat(row_sums, intrsct.getnnz(axis=1))\n",
    "    nnz_j = row_sums[intrsct.indices]\n",
    "\n",
    "    intrsct.data = intrsct.data / np.maximum(nnz_i, nnz_j)\n",
    "    return intrsct.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d75518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAC(tokenList=None, wordVector=None, distance_threshold=0.6, linkage='average'):\n",
    "    if wordVector is None:\n",
    "        wordVector = vectorize(tokenList)\n",
    "\n",
    "    similarityMatrix = calculate_similarity(wordVector)\n",
    "    distanceMatrix = 1 - similarityMatrix\n",
    "\n",
    "    del similarityMatrix\n",
    "\n",
    "    #HAC 層次聚類 Hierarchical Clustering\n",
    "    model = AgglomerativeClustering(distance_threshold=distance_threshold,\n",
    "                                    n_clusters=None,\n",
    "                                    affinity=\"precomputed\",\n",
    "                                    linkage=linkage)\n",
    "    labels = model.fit_predict(distanceMatrix)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60dd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(clustering_data, train_portion):\n",
    "    n_articles = len(clustering_data)\n",
    "    out = np.full(n_articles, -100)  # init\n",
    "\n",
    "    #將結果轉化為稀疏矩陣矩陣的表示方式\n",
    "    tokenList = np.array([x['clean_segment'] for x in clustering_data.values()], dtype=object) \n",
    "\n",
    "    print(f'n_articles: {n_articles}')\n",
    "    print(f'train_portion: {train_portion}')\n",
    "    \n",
    "    stringifyToks = [\" \".join(t) for t in tokenList]\n",
    "    cv = CountVectorizer(binary=True, min_df=1, max_df=0.9, analyzer='word',token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    wv = cv.fit_transform(stringifyToks) #計算詞頻\n",
    "\n",
    "    training_size = int(n_articles * train_portion)\n",
    "    uid_train = np.random.choice(n_articles, training_size, replace=False)\n",
    "    uid_train = np.sort(uid_train)\n",
    "    uid_test = np.array(list(set(range(n_articles)) - set(uid_train)))\n",
    "    \n",
    "    wv_train = wv[uid_train, :]\n",
    "    wv_test = wv[uid_test, :]\n",
    "    \n",
    "    # 層次聚類 Hierarchical Clustering，產生LABEL\n",
    "    distance_threshold = 0.6\n",
    "    labels = HAC(wordVector=wv_train, distance_threshold=distance_threshold)\n",
    "    \n",
    "    countsof = Counter(labels)\n",
    "    y_train = np.array([x if countsof[x] > 1 else -1 for x in labels]) #取value\n",
    "    out[uid_train] = y_train\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "    knn.fit(wv_train, y_train)\n",
    "\n",
    "    y_pred = knn.predict(wv_test)\n",
    "    out[uid_test] = y_pred\n",
    "    \n",
    "    uid_leftover = np.where(out == -1)[0]\n",
    "    wv_leftover = wv[uid_leftover, :]\n",
    "\n",
    "    labels_leftover = HAC(wordVector=wv_leftover, distance_threshold=distance_threshold)\n",
    "    labels_leftover = (np.max(out) + 1) + labels_leftover\n",
    "\n",
    "    out[uid_leftover] = labels_leftover\n",
    "    id2label = dict(zip(clustering_data.keys(), out))\n",
    "    \n",
    "    label_df = pd.DataFrame.from_dict(clustering_data, orient='index')\n",
    "    label_df['label'] = label_df.index.to_series().apply(lambda x: id2label.get(x))\n",
    "    label_df = label_df.reset_index()\n",
    "    label_df = label_df.rename(columns={'index':'doc_id'})\n",
    "#     label_df = label_df[label_df['label'].isna()==False]\n",
    "\n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "end = time.process_time()\n",
    "\n",
    "print('loading...')\n",
    "print(file.stem)\n",
    "\n",
    "try:\n",
    "    data_df = pd.read_csv(root_path/'cofacts_clean_wordseg_20220322-20220422.csv')\n",
    "except:\n",
    "    data_df = pd.read_csv(root_path/'cofacts_clean_wordseg_20220322-20220422.csv', lineterminator='\\n')    \n",
    "\n",
    "clustering_data = {}\n",
    "for rows in data_df[data_df.coexist=='yes'].itertuples():\n",
    "    clustering_data[rows.doc_id] = {'clean_segment': rows.clean_segment}\n",
    "\n",
    "print('clustering...')\n",
    "label_df = clustering(clustering_data, 0.2)\n",
    "\n",
    "label_df.to_csv(out_path/f'cofacts_clustering_label_20220322-20220422.csv', index=False)\n",
    "\n",
    "print(\"This time is being calculated\")\n",
    "print(end - start)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
