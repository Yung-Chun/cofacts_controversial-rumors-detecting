{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e4012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from igraph import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import io\n",
    "import requests\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "from opencc import OpenCC \n",
    "cc = OpenCC('s2t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e87c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = ###deleted for security reasons###\n",
    "out_path = ###deleted for security reasons###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c70b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cofacts_truth_score = pd.read_csv(out_path/'cofacts_truth_score_20220319-20220513.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dedae",
   "metadata": {},
   "source": [
    "# HAC+KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a58c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokenList):\n",
    "    stringifyToks = [\" \".join(t) for t in tokenList]\n",
    "    cv = CountVectorizer(binary=True, min_df=1, max_df=0.9)\n",
    "    wordVector_csr = cv.fit_transform(stringifyToks) #計算詞頻\n",
    "    return wordVector_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40edae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(mat):\n",
    "\n",
    "    if not scipy.sparse.issparse(mat):\n",
    "        mat = scipy.sparse.csr_matrix(mat)\n",
    "    mat = mat.astype(float)\n",
    "\n",
    "    intrsct = mat * mat.T\n",
    "\n",
    "    # for rows\n",
    "    row_sums = mat.getnnz(axis=1)\n",
    "    nnz_i = np.repeat(row_sums, intrsct.getnnz(axis=1))\n",
    "    nnz_j = row_sums[intrsct.indices]\n",
    "\n",
    "    intrsct.data = intrsct.data / np.maximum(nnz_i, nnz_j)\n",
    "    return intrsct.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d75518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAC(tokenList=None, wordVector=None, distance_threshold=0.6, linkage='average'):\n",
    "    if wordVector is None:\n",
    "        wordVector = vectorize(tokenList)\n",
    "\n",
    "    similarityMatrix = calculate_similarity(wordVector)\n",
    "    distanceMatrix = 1 - similarityMatrix\n",
    "\n",
    "    del similarityMatrix\n",
    "\n",
    "    model = AgglomerativeClustering(distance_threshold=distance_threshold,\n",
    "                                    n_clusters=None,\n",
    "                                    affinity=\"precomputed\",\n",
    "                                    linkage=linkage)\n",
    "    labels = model.fit_predict(distanceMatrix)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b60dd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(clustering_data, train_portion):\n",
    "    n_articles = len(clustering_data)\n",
    "    out = np.full(n_articles, -100)  # init\n",
    "\n",
    "    tokenList = np.array([x['clean_segment'] for x in data.values()], dtype=object) \n",
    "\n",
    "    print(f'n_articles: {n_articles}')\n",
    "    print(f'train_portion: {train_portion}')\n",
    "    \n",
    "    stringifyToks = [\" \".join(t) for t in tokenList]\n",
    "    cv = CountVectorizer(binary=True, min_df=1, max_df=0.9, analyzer='word',token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    wv = cv.fit_transform(stringifyToks) \n",
    "\n",
    "    training_size = int(n_articles * train_portion)\n",
    "    uid_train = np.random.choice(n_articles, training_size, replace=False)\n",
    "    uid_train = np.sort(uid_train)\n",
    "    uid_test = np.array(list(set(range(n_articles)) - set(uid_train)))\n",
    "    \n",
    "    wv_train = wv[uid_train, :]\n",
    "    wv_test = wv[uid_test, :]\n",
    "    \n",
    "    distance_threshold = 0.6\n",
    "    labels = HAC(wordVector=wv_train, distance_threshold=distance_threshold)\n",
    "    \n",
    "    countsof = Counter(labels)\n",
    "    y_train = np.array([x if countsof[x] > 1 else -1 for x in labels]) #取value\n",
    "    out[uid_train] = y_train\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "    knn.fit(wv_train, y_train)\n",
    "\n",
    "    y_pred = knn.predict(wv_test)\n",
    "    out[uid_test] = y_pred\n",
    "    \n",
    "    uid_leftover = np.where(out == -1)[0]\n",
    "    wv_leftover = wv[uid_leftover, :]\n",
    "\n",
    "    labels_leftover = HAC(wordVector=wv_leftover, distance_threshold=distance_threshold)\n",
    "    labels_leftover = (np.max(out) + 1) + labels_leftover\n",
    "\n",
    "    out[uid_leftover] = labels_leftover\n",
    "    id2label = dict(zip(clustering_data.keys(), out))\n",
    "    \n",
    "    label_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    label_df['label'] = label_df.index.to_series().apply(lambda x: id2label.get(x))\n",
    "#     label_df = label_df.reset_index()\n",
    "#     label_df = label_df.rename(columns={'index':'doc_id'})\n",
    "    label_df = label_df[label_df['label'].isna()==False]\n",
    "\n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3ed9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "cofacts_covid_20220319-20220513\n",
      "clustering...\n",
      "n_articles: 6509\n",
      "train_portion: 0.1\n",
      "cofacts_vaccine_20220319-20220513\n",
      "clustering...\n",
      "n_articles: 2776\n",
      "train_portion: 0.1\n",
      "cofacts_coexist_20220319-20220513\n",
      "clustering...\n",
      "n_articles: 103\n",
      "train_portion: 0.1\n",
      "cofacts_rapid_test_20220319-20220513\n",
      "clustering...\n",
      "n_articles: 573\n",
      "train_portion: 0.1\n",
      "This time is being calculated\n",
      "7.000000000090267e-05\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "end = time.process_time()\n",
    "\n",
    "sub = ['covid', 'coexist', 'vaccine', 'rapid_test']\n",
    "print('loading...')\n",
    "for file in root_path.glob(f'*cofacts*'):\n",
    "    for x in sub:\n",
    "        if x in file.stem:\n",
    "            print(file.stem)\n",
    "            with open(file , 'r', encoding='big5') as reader:\n",
    "                data = json.loads(reader.read())\n",
    "\n",
    "            print('clustering...')\n",
    "            label_df = clustering(data, 0.1)\n",
    "            label_df.label = label_df.label.astype(str)\n",
    "            label_df.label = label_df.category + '_' + label_df.label\n",
    "            label_df = label_df.merge(cofacts_truth_score[['article_id', 'article_type_count']], how='left')\n",
    "            label_df.to_csv(out_path/f'{file.stem}_clustering.csv', index=False)\n",
    "\n",
    "print(\"This time is being calculated\")\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "492bbbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>clean_segment</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>article_type_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ewp9hkii5mcy</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>[補充, 湖北省, 武漢, 華中, 科技, 大學, 同濟, 醫學院, 附屬, 同濟, 醫院,...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_508</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4rj2rxep739</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>[武漢, 肺炎, 傳染力, 潛伏期, 尚未, 發病, 看似, 症狀, 狀態, 已經, 傳染,...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_373</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20ewmsj8w05j2</td>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>[武漢, 肺炎, 證實, 新型, 目前, 泰國, 日本, 越南, 確診, 病例, 中國, 政...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_318</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3cpvcucef9sgv</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>[耳鼻喉科, 醫師, 羣組, 裡面, 大部分, 開業, 醫師, 認為, 這些, 臺灣, 最多...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_399</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2vpo1u9e1fat5</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>[共同, 開發, 武漢, 肺炎, 疫苗, 共同, 中央社, 記者, 陳韻聿, 臺北, 臺灣,...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_491</td>\n",
       "      <td>不是謠言</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1brwzm6np570e</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>[不要, 相信, 鼻用, 篩戳, 喉嚨, 或者, 鼻孔, 酒精, 不會, 確診, 網路, 謠...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_43</td>\n",
       "      <td>不是謠言</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>2bgsy3fwz0s6h</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>[無能, 還是, 人謀不臧, 國內, 篩試劑, 國際, 衛福部長, 陳時中, 因為, 臺灣,...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_111</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>34emnzhqxrmq9</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>[自己, 快篩, 功能, 測試, 住呼吸, 多少, 正常, 超強]</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_42</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>f5vbbwot1z8m</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>[唾液, 覈准, 進口, 扯上, 高端, 陳時中, 高端, 什麼, 關係, 只要, 顏色, ...</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>3g2juejernwyb</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>[正港, 臺灣製, 快篩劑, 香港, 售價, 新臺幣]</td>\n",
       "      <td>rapid_test</td>\n",
       "      <td>rapid_test_19</td>\n",
       "      <td>謠言或個人意見</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id   createdAt  \\\n",
       "0    1ewp9hkii5mcy  2020-01-28   \n",
       "1     c4rj2rxep739  2020-01-23   \n",
       "2    20ewmsj8w05j2  2020-01-19   \n",
       "3    3cpvcucef9sgv  2020-02-24   \n",
       "4    2vpo1u9e1fat5  2020-02-24   \n",
       "..             ...         ...   \n",
       "568  1brwzm6np570e  2022-05-01   \n",
       "569  2bgsy3fwz0s6h  2022-05-01   \n",
       "570  34emnzhqxrmq9  2022-05-01   \n",
       "571   f5vbbwot1z8m  2022-05-01   \n",
       "572  3g2juejernwyb  2022-05-01   \n",
       "\n",
       "                                         clean_segment    category  \\\n",
       "0    [補充, 湖北省, 武漢, 華中, 科技, 大學, 同濟, 醫學院, 附屬, 同濟, 醫院,...  rapid_test   \n",
       "1    [武漢, 肺炎, 傳染力, 潛伏期, 尚未, 發病, 看似, 症狀, 狀態, 已經, 傳染,...  rapid_test   \n",
       "2    [武漢, 肺炎, 證實, 新型, 目前, 泰國, 日本, 越南, 確診, 病例, 中國, 政...  rapid_test   \n",
       "3    [耳鼻喉科, 醫師, 羣組, 裡面, 大部分, 開業, 醫師, 認為, 這些, 臺灣, 最多...  rapid_test   \n",
       "4    [共同, 開發, 武漢, 肺炎, 疫苗, 共同, 中央社, 記者, 陳韻聿, 臺北, 臺灣,...  rapid_test   \n",
       "..                                                 ...         ...   \n",
       "568  [不要, 相信, 鼻用, 篩戳, 喉嚨, 或者, 鼻孔, 酒精, 不會, 確診, 網路, 謠...  rapid_test   \n",
       "569  [無能, 還是, 人謀不臧, 國內, 篩試劑, 國際, 衛福部長, 陳時中, 因為, 臺灣,...  rapid_test   \n",
       "570                  [自己, 快篩, 功能, 測試, 住呼吸, 多少, 正常, 超強]  rapid_test   \n",
       "571  [唾液, 覈准, 進口, 扯上, 高端, 陳時中, 高端, 什麼, 關係, 只要, 顏色, ...  rapid_test   \n",
       "572                        [正港, 臺灣製, 快篩劑, 香港, 售價, 新臺幣]  rapid_test   \n",
       "\n",
       "              label article_type_count  \n",
       "0    rapid_test_508            謠言或個人意見  \n",
       "1    rapid_test_373            謠言或個人意見  \n",
       "2    rapid_test_318            謠言或個人意見  \n",
       "3    rapid_test_399            謠言或個人意見  \n",
       "4    rapid_test_491               不是謠言  \n",
       "..              ...                ...  \n",
       "568   rapid_test_43               不是謠言  \n",
       "569  rapid_test_111            謠言或個人意見  \n",
       "570   rapid_test_42            謠言或個人意見  \n",
       "571   rapid_test_60                NaN  \n",
       "572   rapid_test_19            謠言或個人意見  \n",
       "\n",
       "[573 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89185c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
